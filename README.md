# Credit Risk Analysis

● Developed a model to classify safe and unsafe credit card applicants, forecast and detect outliers by building a supervised predictive logistic model. <br>
● Implemented Bagging and Boosting techniques as well as other Ensemble methods.

A Quick Look: <br>
The German Credit Dataset was used for this project. <br>
1. Missing Values Manipulation <br>
2. Univariable Analysis, Correlation Matrix <br>
3. Logistic Regression<br>
4. Decision Tree Modeling <br>
5. Lift Curve<br>
6. Confusion Table<br>
7. AUC and ROC Curves<br>
7. Getting the 'scores' from applying the model to the data<br>
8. Decision Trees<br>
9. Random Forest<br>
10. Prediction, Evaluation and Performance Measures<br>
11. Cost Matrix <br>


Final Verdict: <br>
The best fit Decision Tree model based on analysis is the Model2. Now comparing the performance of the best fit Decision Tree Model with the best fit Random Forest model obtained above, we notice that the Random Forest model has a better accuracy than the Decision tree model.  The accuracy of the Random Forest model is 0.7772 where as the accuracy of the Decision Tree model is 0.6383. <br>

The ROC plots have been plotted for the best fit Random Forest Model and the best fit Decision Tree model. It can be seen that the false positives increases with in Decision Tree model where as in the Random Forest model it remains constant. <br>

Average profit graphs have been plotted for the best fit Random Forest model and the best fit Decision Tree model.  The max-profit obtained for the Random Forest model is 3400 where as for the Decision Tree model it is 1200. This further proves that the Random Forest method is a more efficient method than the Decision Tree method.<br>


Do read the report on my analysis. <br>



Thank you for stopping by! <br>
Have a great rest of the day! <br>
